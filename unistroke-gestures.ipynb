{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "from scipy.signal import resample\n",
    "\n",
    "from recognizer import Recogniser, Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_POINTS = 50\n",
    "INPUT_PARAMETERS = 2\n",
    "\n",
    "SCRIPT_DIR = os.path.abspath('') #os.path.dirname(__file__); workaround for jupyter notebook\n",
    "\n",
    "TRAIN_PATH = os.path.join(SCRIPT_DIR, \"dataset/train\")\n",
    "TEST_PATH = os.path.join(SCRIPT_DIR, \"dataset/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data: list[tuple[str, list[tuple]]] = []\n",
    "test_data: list[tuple[str, list[tuple]]] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path: str, data_list: list) -> None:\n",
    "  for root, _, files in os.walk(path):\n",
    "    if len(files) == 0:\n",
    "      continue\n",
    "\n",
    "    for file_name in files:\n",
    "      if not file_name.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "      class_name = os.path.basename(root)\n",
    "      file_path = os.path.join(path, class_name, file_name)\n",
    "\n",
    "      data = pd.read_csv(filepath_or_buffer=file_path, delimiter=\",\")\n",
    "\n",
    "      points = data[[\"x\", \"y\"]]\n",
    "      points = np.array(points, dtype=float)\n",
    "\n",
    "      scaler = StandardScaler()\n",
    "      points = scaler.fit_transform(points)\n",
    "\n",
    "      points_resampled = resample(points, SAMPLE_POINTS)\n",
    "\n",
    "      data_list.append((class_name, points_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(TRAIN_PATH, train_data)\n",
    "get_data(TEST_PATH, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data: list[tuple[str, list[tuple]]]) -> tuple[np.ndarray, np.ndarray, list[str]]:\n",
    "  labels = [sample[0] for sample in data]\n",
    "  print(set(labels))\n",
    "\n",
    "  encoder = LabelEncoder()\n",
    "  labels_encoded = encoder.fit_transform(labels)\n",
    "\n",
    "  print(set(labels_encoded))\n",
    "\n",
    "  y = to_categorical(labels_encoded)\n",
    "\n",
    "  print(len(y[0]))\n",
    "\n",
    "  sequences = [sample[1] for sample in data]\n",
    "  X = np.array(sequences)\n",
    "\n",
    "  return X, y, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_mark', 'arrow', 'left_curly_brace', 'x', 'rectangle', 'caret', 'check', 'star', 'delete_mark', 'right_sq_bracket', 'triangle', 'left_sq_bracket', 'pigtail', 'right_curly_brace', 'circle', 'v'}\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}\n",
      "16\n",
      "{'question_mark', 'arrow', 'left_curly_brace', 'x', 'rectangle', 'caret', 'check', 'star', 'delete_mark', 'right_sq_bracket', 'triangle', 'left_sq_bracket', 'pigtail', 'right_curly_brace', 'circle', 'v'}\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}\n",
      "16\n",
      "(5120, 50, 2) (160, 50, 2) (5120, 16) (160, 16)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, labels_train = split_data(train_data)\n",
    "X_test, y_test, labels_test = split_data(test_data)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) LSTM Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:   \n",
    "\n",
    "  def init(self, options: dict) -> None:\n",
    "    self.model = Sequential()\n",
    "\n",
    "    self.model.add(LSTM(options[\"lstm_neurons\"], input_shape=(SAMPLE_POINTS, INPUT_PARAMETERS)))\n",
    "    self.model.add(Dense(options[\"fully_connected_neurons\"], activation=options[\"fully_connected_activation_function\"]))\n",
    "    if options[\"add_dropout\"]:\n",
    "      self.model.add(Dropout(options[\"dropout_rate\"]))\n",
    "    self.model.add(Dense(len(set(labels_train)), activation='softmax'))\n",
    "\n",
    "    self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    self.reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=options[\"learn_rate\"])\n",
    "    self.stop_early = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "  def train(self, X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray, options: dict) -> None:\n",
    "    # Train the model\n",
    "    self.history = self.model.fit(\n",
    "      X_train,\n",
    "      y_train,\n",
    "      epochs=options[\"epochs\"],\n",
    "      batch_size=options[\"batch_size\"],\n",
    "      validation_data=(X_test, y_test),\n",
    "      verbose=1,\n",
    "      callbacks=[self.reduce_lr, self.stop_early]\n",
    "    )\n",
    "\n",
    "  def summarise(self) -> None:\n",
    "    self.model.summary()\n",
    "\n",
    "  def plot(self) -> None:\n",
    "    loss = self.history.history['loss']\n",
    "    val_loss = self.history.history['val_loss']\n",
    "    accuracy = self.history.history['accuracy']\n",
    "    val_accuracy = self.history.history['val_accuracy']\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 7))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Accuracy (Line), Loss (Dashes)')\n",
    "\n",
    "    ax.axhline(1, color='gray')\n",
    "\n",
    "    plt.plot(accuracy, color='blue')\n",
    "    plt.plot(val_accuracy, color='orange')\n",
    "    plt.plot(loss, '--', color='blue', alpha=0.5)\n",
    "    plt.plot(val_loss, '--', color='orange', alpha=0.5)\n",
    "\n",
    "  def predict(self, X_test: np.ndarray, y_test: np.ndarray, labels_train: list[str]) -> int:\n",
    "    # let the model make predictions for our training data\n",
    "    t1 = time.time()\n",
    "    y_predictions = self.model.predict(X_test)\n",
    "    t2 = time.time()\n",
    "    \n",
    "\n",
    "    # to build a confusion matrix, we have to convert it to classifications\n",
    "    # this can be done by using the argmax() function to set the probability to 1 and the rest to 0\n",
    "    y_predictions = np.argmax(y_predictions, axis=1)\n",
    "\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # create and plot confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test_labels, y_predictions)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    ConfusionMatrixDisplay(conf_matrix, display_labels=set(labels_train)).plot(ax=plt.gca())\n",
    "\n",
    "    plt.xticks(rotation=90, ha='center')\n",
    "    \n",
    "    return t2-t1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which hyperparameters make sense to be reduced:\n",
    "- number of lstm units (default: 64)\n",
    "- dropout rate (default: not included) -> add to reduce learning rate\n",
    "- fully connected neurons (default: 32)\n",
    "- sequence length (default: 100%)\n",
    "- batch size (default: 32)\n",
    "- epochs (default: 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "  \"lstm_neurons\": 64,\n",
    "  \"fully_connected_neurons\": 32,\n",
    "  \"fully_connected_activation_function\": \"relu\",\n",
    "  \"epochs\": 10,\n",
    "  \"batch_size\": 32,\n",
    "  \"learn_rate\": 0.0001,\n",
    "  \"add_dropout\": False,\n",
    "  \"dropout_rate\": 0.2\n",
    "}\n",
    "\n",
    "nn1 = NN()\n",
    "nn1.init(options)\n",
    "nn1.train(X_train, y_train, X_test, y_test, options)\n",
    "nn1.summarise()\n",
    "nn1.plot()\n",
    "iteration_1_time = nn1.predict(X_test, y_test, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn1.model.save(\"trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"labels_model.txt\", \"w\") as f:\n",
    "  f.write(str(list(set(labels_train))))\n",
    "  f.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iteration 2\n",
    "\n",
    "first, lets reduce the number of neurons from lstm and fully conncected. this should greatly reduce the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "  \"lstm_neurons\": 32,\n",
    "  \"fully_connected_neurons\": 16,\n",
    "  \"fully_connected_activation_function\": \"relu\",\n",
    "  \"epochs\": 10,\n",
    "  \"batch_size\": 32,\n",
    "  \"learn_rate\": 0.0001,\n",
    "  \"add_dropout\": False,\n",
    "  \"dropout_rate\": 0.2\n",
    "}\n",
    "\n",
    "nn2 = NN()\n",
    "nn2.init(options)\n",
    "nn2.train(X_train, y_train, X_test, y_test, options)\n",
    "nn2.summarise()\n",
    "nn2.plot()\n",
    "iteration_2_time = nn2.predict(X_test, y_test, labels_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iteration 3\n",
    "\n",
    "trying to reduce the number of neurons again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "  \"lstm_neurons\": 16,\n",
    "  \"fully_connected_neurons\": 8,\n",
    "  \"fully_connected_activation_function\": \"relu\",\n",
    "  \"epochs\": 10,\n",
    "  \"batch_size\": 32,\n",
    "  \"learn_rate\": 0.0001,\n",
    "  \"add_dropout\": False,\n",
    "  \"dropout_rate\": 0.2\n",
    "}\n",
    "\n",
    "nn3 = NN()\n",
    "nn3.init(options)\n",
    "nn3.train(X_train, y_train, X_test, y_test, options)\n",
    "nn3.summarise()\n",
    "nn3.plot()\n",
    "iteration_3_time = nn3.predict(X_test, y_test, labels_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iteration 4\n",
    "\n",
    "reducing the lstm units to 16 was a bit too much. the accary droped quite some bit.\n",
    "we again pick the number of neurons from iteration 3 and now change the epoch size/batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "  \"lstm_neurons\": 32,\n",
    "  \"fully_connected_neurons\": 16,\n",
    "  \"fully_connected_activation_function\": \"relu\",\n",
    "  \"epochs\": 5,\n",
    "  \"batch_size\": 16,\n",
    "  \"learn_rate\": 0.0001,\n",
    "  \"add_dropout\": False,\n",
    "  \"dropout_rate\": 0.2\n",
    "}\n",
    "\n",
    "nn4 = NN()\n",
    "nn4.init(options)\n",
    "nn4.train(X_train, y_train, X_test, y_test, options)\n",
    "nn4.summarise()\n",
    "nn4.plot()\n",
    "iteration_4_time = nn4.predict(X_test, y_test, labels_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iteration 5\n",
    "\n",
    "picking the parameters from iteration 4 and reducing learn rate along with adding a dropout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "  \"lstm_neurons\": 32,\n",
    "  \"fully_connected_neurons\": 16,\n",
    "  \"fully_connected_activation_function\": \"relu\",\n",
    "  \"epochs\": 5,\n",
    "  \"batch_size\": 16,\n",
    "  \"learn_rate\": 0.0025,\n",
    "  \"add_dropout\": True,\n",
    "  \"dropout_rate\": 0.5\n",
    "}\n",
    "\n",
    "nn5 = NN()\n",
    "nn5.init(options)\n",
    "nn5.train(X_train, y_train, X_test, y_test, options)\n",
    "nn5.summarise()\n",
    "nn5.plot()\n",
    "iteration_5_time = nn5.predict(X_test, y_test, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iteration_1_time, iteration_2_time, iteration_3_time, iteration_4_time, iteration_5_time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) 1$ Recogniser Approach\n",
    "\n",
    "we add the first of each class from the train set and add it as a template to the 1$ recogniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec1 = Recogniser(use_predefined_templates=False)\n",
    "templates_added = []\n",
    "\n",
    "for data_point in train_data:\n",
    "  if not data_point[0] in templates_added:\n",
    "    points = []\n",
    "    for p in data_point[1]:\n",
    "      points.append(Point(p[0], p[1]))\n",
    "\n",
    "    if not rec1.add_template(data_point[0], points):\n",
    "      continue\n",
    "    \n",
    "    templates_added.append(data_point[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(len(rec1.templates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_points(data):\n",
    "  data_points = []\n",
    "  \n",
    "  for p in data:\n",
    "    class_name = p[0]\n",
    "    class_points = []\n",
    "\n",
    "    for i in p[1]:\n",
    "      class_points.append(Point(i[0], i[1]))\n",
    "\n",
    "    data_points.append((class_name, class_points))\n",
    "  \n",
    "  return data_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = prepare_points(test_data)\n",
    "results = []\n",
    "\n",
    "for p in points:\n",
    "  t1 = time.time()\n",
    "  res = rec1.recognise(p[1])\n",
    "  t2 = time.time()\n",
    "\n",
    "  results.append({\n",
    "    \"predicted_class\": res[0].name,\n",
    "    \"actual_class\": p[0],\n",
    "    \"inference_time\": t2-t1,\n",
    "    \"accuracy\": res[1],\n",
    "  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollar_accuracy = 0\n",
    "dollar_inference_time = 0\n",
    "\n",
    "for r in results:\n",
    "  if r[\"predicted_class\"] == r[\"actual_class\"]:\n",
    "    dollar_accuracy += 1\n",
    "  \n",
    "  dollar_inference_time += r[\"inference_time\"]\n",
    "\n",
    "dollar_accuracy = dollar_accuracy / len(results)\n",
    "dollar_inference_time = dollar_inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5875 6.5093584060668945\n"
     ]
    }
   ],
   "source": [
    "print(dollar_accuracy, dollar_inference_time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "| Approach | Inference Time (s) | Accuracy (%) | LSTM Neurons | FC Neurons | Epochs | Batch Size | Learn Rate | Dropout |\n",
    "|--------------|-----------|------------|--------------|-----------|------------|--------------|-----------|------------|\n",
    "| Iteration 1 (LSTM-NN) | 0.53 | 0.989 | 64 | 32 | 10 | 32 | 0.0001 | False |\n",
    "| Iteration 2 (LSTM-NN) | 0.43 | 0.994 | 32 | 16 | 10 | 32 | 0.0001 | False |\n",
    "| Iteration 3 (LSTM-NN) | 0.44 | 0.953 | 16 | 8 | 10 | 32 | 0.0001 | False |\n",
    "| Iteration 4 (LSTM-NN) | 0.48 | 0.981 | 32 | 16 | 5 | 16 | 0.0001 | False |\n",
    "| Iteration 5 (LSTM-NN) | 0.58 | 0.730 | 32 | 16 | 5 | 16 | 0.0025 | 0.5 |\n",
    "| $1 Recogniser  (1 template of each class) | 4.17 | 0.775 | -- | -- | -- | -- | -- | -- |\n",
    "| $1 Recogniser  (1 template of each class[mirrored]) | 6.51 | 0.588 | -- | -- | -- | -- | -- | -- |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### findings:\n",
    "\n",
    "- adding more templates to 1$ recogniser might increase accuracy although it would also further increase inference time which is already pretty high.\n",
    "- 1$ templates have a big limitation e.g. recognition of a circle works good if it was drawn clockwise, but not at all if it was drawn counter-clockwise. this indicates that at least 2 templates for each class are required!\n",
    "- inference time for lstm-nn is roughly the same for all iterations and we could greatly reduce the number of parameters from 19,760 to 5,280.\n",
    "- adding a dropout layer and changing min learn rate let accuracy drop to a unusable level (0.73) (requires more iterations and fine-tuning).\n",
    "- neural network requires more computing power (ram, gpu/cpu) for training and keeping the model in memory whereas $1 recogniser almost requires no memory.\n",
    "- adding mirrored templates to 1$ recogniser lets the accuracy drop frin 77% to 58%. this could be due to too many templates that need to be checked.\n",
    "\n",
    "\n",
    "### conclusion:\n",
    "it shows that one template per class for the 1$ recogniser is insufficient and results in a bad accuracy. adding mirrored templates (so 32 templates alltogether) results in bad accuracy. it also requires a longer time to predict a gesture compared with multiple iterations of lstm-nn. considering that todays computers have dedicated hardware for neural networks (smartphones and macs have dedicated cpus), i see no point why anyone would not choose a lstm-nn - they predict faster, can be optimised reducing hardware requirements, models can be saved and loaded, and accuracy is almost 100\\%. the only advantage of 1$ recogniser is that it is light because it only requires the math package from the standard library whereas lstm-nn requires a few dozen packages.\n",
    "Alltogether, I would choose Iteration 2 because it has almost 100% accuracy and requires the least amount of inference time. For smaller applications where accuracy is not critical, $1 recogniser is good enough; if you train with vertically mirrored templates and few templates. It also removes a lot of startup time, because there is only math package required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
